\section{Introduction}
%introduction to 3D shape reconstruction from single view
Inferring 3D shape from a single view image is a traditional problem for computer vision. In computer graphics, 3D modeling with a given image has also been extensively studied. \comments{Though it have been studied for decades, the problem remains challenging, due to the fact that 3D-to-2D projection is not invertible, and large portions of the 3D shape features are excluded in the 2D image.} In recent years, deep neural networks~\cite{3DR2N2,PSGN} \todo{add more refs of SVR} have achieved great success in this field. Unlike classic shape from X \todo{add refs of shape from X} approaches, these neural networks are able to recover not only the visible frontal shape but also the invisible part for object from a single view color image by learning complicate prior knowledge from dataset. 

These networks all rely on variants of convolution neural network to extract information and encode 2D images, but use quite different techniques to represent and decode 3D shapes. Started by 3D ShapeNets\cite{3dshapenet} and greatly improved by introducing octree structure\cite{octreegen}, volumetric representation and 3D convolution networks are most commonly used in this problem. There is also point set generation network\cite{PSGN} that use unordered point set representation and directly regress point set using both convolution and fully connected breaches. Other interesting approaches includes \cite{endface} which use bilinear model to represent shape of face and regress the interpolation coefficient to generate the shape and \cite{surfnet} which explicitly employ spherical parameterization as post-processing to represent shape as geometry image in parameter domain and so on.

%introduction to 3d mesh reconstruction networks-- AtlasNet and Pixel2Mesh to be specific
In latest works, AtlasNet\cite{atlasnet} and Pixel2Mesh\cite{pixel2mesh} to be specific, a new idea have been applied on this problem, which let the network learns to map from a predefined surface (square and sphere for AtlasNet and ellipsoid for Pixel2Mesh) to target surface instead of directly regress the absolute positions of surface points as in \cite{PSGN}. In such networks, the mesh can be generated by simply transfer the triangulation from the predefined surface to the target surface. More importantly, the predefined surface can serve as the domain of definition allowing mesh related operations and energy functions to be integrated into neural networks (The Laplacian regularization term from Pixel2Mesh is one example of such case). Therefore, we think that such approaches are intriguing and worth our follow-up.

%introduction to the self-intersection issue
In this paper, we address a specific issue that appears in both AtlasNet\cite{atlasnet} and Pixel2Mesh\cite{pixel2mesh}. As shown in Figure\todo{add the figure to show the issue of self-intersected surfaces}, the AtlasNet and Pixel2Mesh will generate mesh with self-intersected surface. This issue appears partially because AtlasNet and Pixel2Mesh directly employed the Chamfer distance loss from point set generation network(PSGN)\cite{PSGN}. The Chamfer distance loss was designed to measure the discrepancy between two unordered point set and it does not take surface into consideration. More importantly, as we will show in Sec~\ref{subsec:inj}, the essential reason behind this issue is that the learned mapping from the predefined surface to target surface is non-injective. In other words, two points on the predefined surface can be mapped to same point by the network causing the generated surface to be self-intersected and self-overlapped. 

%our proposed solution 
Being injective is difficult to enforce in parameterization optimization and mesh deformation. It has been studied for many years in computer graphics.\todo{add refs about injective mapping}. One possible strategy is to start from a feasible solution and keep every optimization iteration or deformation inside feasible region. This strategy is impractical in training neural networks, since the network is simultaneously learning to predict the mapping for many different shapes and only a batch of these shapes are sampled from the dataset in each iteration. We propose a regularization technique to help enforcing the learning of injective mapping for mesh. In practice, we employ an additional inverse 3D decoder to learn to predict an inverse mapping from target surface back to the predefined surface. Therefore, a point from predefined surface can be mapped to target surface and then mapped back. We use difference after such cycle mapping to form our regularization term and we call it cycle regularization. Note that the inverse 3D decoder is only needed in training phase, therefore it is only a part of the regularization technique and do not increase the complexity of the original neural network. As we will explain in Sec~\ref{subsec:cyclereg} such inverse 3D decoder can help forcing original network to be as injective as possible, therefore help solving the issue we addressed .

\todo{summarize the contribution}
 