 \section{Introduction}
Inferring 3D shape from a single perspective is a traditional problem for computer vision.  In computer graphics, 3D modeling with a given image has also been studied. Though it have been studied for tens of years, the problem remains challenging in both fields. The challenge arises from the fact that 3D-to-2D projection is not invertible and large portions of the 3D shape features are excluded in the 2D image. Recently, great success has been achieved for 3d shape generation from a single color image using deep learning techniques
\cite{3DR2N2,PSGN}. By the using convolutional layers on regular grids or multi-layer
perception on unordered 3D coordinates, the estimated 3D shape is represented
as either a volume occupancy \cite{3DR2N2} or point cloud \cite{PSGN} in neural networks. However, both representations lose important
surface details, and is non-trivial to recover continue surface from.

As a matter of fact, mesh is a more desirable form of 3D shape representation for many applications in computer graphics, since it is capable of
modeling shape details, easy to deform for animation, ready to render with various surface material...


Effort have been made by works like\cite{endface}, which integrate the technique of 3D morphable model from computer graphics into neural network to make an end-to-end trainable network that can output 3D mesh. However, such network can only ouput mesh for a specific class of object (the specific class is human face in \cite{endface}).

In order to develop an end-to-end trainable network that can output mesh for multiple class of objects, we propose a brand new framework of neural network in this paper. Our framework is composed of two neural networks, the parameterization network and the semantic network.  The semantic network predict parameters for the parameterization network which maps the unit sphere surface to the target surface. Such network structure can enable the representation of shape as spatial distribution. This perspective is inspired by VAE\cite{VAE} in the sense that a large portion of distribution can be approximated by standard normal distribution plus a learned complicate mapping. In our framework, we actually represent the output shape using a uniform distribution defined on sphere surface plus a predicted complicate mapping (carried out by the parameterization network). These two combined can represent complicate distribution of surface. Therefore, such representation can enable the network to generate mesh as output and enable the mesh related operation and losses for the network.

In summary, our contributions are
\begin{itemize}
	\item  Introduction of parameterization network that enable mesh generation by representing the 3D shape as a spherical uniform distribution plus a learned/predicted mapping.
	\item  Exploring the idea that use semantic network to predict parameters for parameterization network. Such structure relate input image to parameterization network.
	\item Though bijective is not ensured for the predicted mapping now, with the ability to integrate mesh based operation and losses, our framework push the neural network towards an actual \emph{parameterization prediction} network that would allow more mesh operation former studied in computer graphics to be integrated into neural networks.
\end{itemize}          

