\section{Introduction}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{img/issue/issue}
	\caption{The issue of the self-intersection in 3D surface mesh reconstruction networks: (a) A surface mesh of a plane generated by Pixel2Mesh \cite{pixel2mesh}. (b) A surface mesh of a plane generated by AtlasNet \cite{atlasnet}(sphere as its parameter domain). The triangle faces are rendered as golden on outside and bluish on inside. The inside triangles are exposed due to the issue of self-intersection. Some examples of the issue are also highlighted in zoomed view.}
	\label{fig:issue}
\end{figure}
%introduction to 3D shape reconstruction from single view
Inferring 3D shape from a single view image is a traditional problem for computer vision. In computer graphics, 3D modeling with a given image has also been extensively studied. In recent years, deep neural networks (\cite{3DR2N2,PSGN,3Drender,imgrecon15,3dshapenet,endface,octreegen,surfnet,shapeprior}) have achieved great success in this field. Unlike classic shape from X (e.g. \cite{shapefromshading,shapefromtext1,shapefromtext2}) approaches, these neural networks are able to recover not only the visible frontal shape but also the invisible part for object from a single-view color image by learning and representing complicated prior knowledge from a large dataset. 

These networks all rely on variants of 2D convolution neural networks to extract information and encode 2D images, but use quite different techniques to represent and decode 3D shapes. Started by 3D ShapeNets \cite{3dshapenet} and greatly improved by introducing octree structure (\cite{octreegen}), volumetric representation and 3D convolution networks are most commonly used in this problem. There is also a point set generation network (\cite{PSGN}) that uses unordered point set representation and directly regresses point set using both convolution and fully connected branches. \cite{endface} employ a bilinear model to represent 3D faces and regress the interpolation coefficients to generate a face shape from an image. \cite{surfnet} explicitly use spherical parameterization as a post-processing stage to represent a 3D shape as a geometry image in the parameter domain. 

%introduction to 3d mesh reconstruction networks-- AtlasNet and Pixel2Mesh to be specific
In latest works, AtlasNet (\cite{atlasnet}) and Pixel2Mesh (\cite{pixel2mesh}) to be specific, a new idea has been applied on this problem. They let the networks learn to map from a predefined surface (square and sphere for AtlasNet and ellipsoid for Pixel2Mesh) to a target surface instead of directly regress the absolute positions of surface points as in \cite{PSGN}. These methods have shown great potential in generating meshes for generic objects. It is convenient to integrate mesh-related operations and energy functions in these networks. For example, Pixel2Mesh integrates graph-based unpooling and Laplacian regularization in the neural network.

Despite all of these progresses, there are still a lot of issues preventing network-generated meshes from being used in real-world applications. However, we believe deep learning is the most promising approach to integrate more intelligent in 3D modeling .

%introduction to the self-intersection issue
 In this paper, we address a specific issue that appears in both AtlasNet \cite{atlasnet} and Pixel2Mesh \cite{pixel2mesh}. As shown in Figure~\ref{fig:issue}, the AtlasNet and Pixel2Mesh will generate mesh with self-intersected surfaces. This issue appears partially because AtlasNet and Pixel2Mesh employed the Chamfer distance loss , which is used firstly to train the point set generation network (PSGN, \cite{PSGN}). The Chamfer distance loss was designed to measure the discrepancy between two unordered point set and it does not take surface into consideration. The AtlasNet used Poission surface reconstruction as post-processing or double-sided lighting in rendering to cover up this issue. The Pixel2Mesh adopted a coarse-to-fine framework and added several other losses (i.e. edge length loss, Laplacian loss) in order to alleviate this issue. They all failed to address the essential reason behind this issue, after all most effort in these mesh generation networks have been focused on increasing shape details for the generated mesh.

In this paper, we tackle the issue of self-intersection from the essential reason behind it, which is the non-injectivity of the predicted mapping, or in other words, two points on the predefined surface can be mapped to same point by the neural network, causing the generated surface to be self-intersected and self-overlapped. (as we will establish in Sec~\ref{subsec:inj})

%challenge
Constraining the Jacobian of the mapping is used to enforce injectivity. \cite{tvcgprevent} have sucessfully used it to prevent self-intersection in Free-Form Deformation(FFD). Starting from an object that is free from self-intersection, \cite{tvcgprevent} devide FFD into injective sub-steps to ensure the output to be free from self-intersection.

Injectivity has also been studied under parameterization optimization to prevent folding. For surface with disk topology, one possible strategy is also to start from a feasible solution (by Tutte's embedding \cite{tutte} or its variants) and keep every optimization iteration or deformation inside feasible region. This can be enforced by adding barrier energy from distortion metrics (e.g. \cite{provableplanarmapping,lifted_bijection}), bounding the triangle distortion (e.g.\cite{freeboundary,boundeddistortion})
or using a progressive strategy \cite{Liu_PP_2018}. 

We find it difficult to adopt the strategy in these classic works into training neural networks, since the network is simultaneously learning to predict the mapping for many different shapes and only a batch of these shapes are sampled from the dataset in each training iteration. It is not easy to initialize the network parameters to ensure that initial outputs are free of self-intersection for all possible inputs. It is also not easy to alter batch-based optimizer to constrain the deformation of outputs to be inside injective region for all possible inputs. We propose to use regularization technique to help enforcing the learning of injective mapping for mesh. Our technique is easy to implement for neural network by reusing the existing differentiable layers, given an existing surface mesh reconstruction networks as AtlasNet \cite{atlasnet} and Pixel2Mesh \cite{pixel2mesh}.

%our solution
 Our strategy is to use an additional inverse 3D decoder to learn to predict an inverse mapping from target surface back to the predefined surface along with the forward mapping in the original network. Therefore, a point from predefined surface can be mapped to target surface and then mapped back. We use difference after such cycle mapping to form our regularization term and we call it cycle regularization. While the network learning a mapping to approach the target surface, our regularization term is trying to ensure that an inverse mapping exists (i.e making the forward mapping injective, as we explain in Sec~\ref{subsec:cyclereg}).
Note that the inverse 3D decoder is only needed in training phase, therefore it is only a part of the regularization technique and do not increase the complexity of the original neural network.

In summary, our contributions in this paper are:
\begin{itemize}
	\item We propose the cycle regularization technique to prevent self-intersection for surface mesh reconstruction networks. 
	\item We apply our cycle regularization technique on two latest mesh generation networks, AtlasNet \cite{atlasnet} and Pixel2Mesh \cite{pixel2mesh}, showing that our technique keeps the network end-to-end trainable by using existing differentiable layers.
	\item We validate with experiments that when trained with cycle regularization, these networks are able to produce surface meshes with significantly less self-intersection, and still lead to comparable distance between the generated mesh and the ground-truth mesh with original networks. 
\end{itemize}

 