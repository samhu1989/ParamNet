\begin{abstract}
	We propose an end-to-end deep learning framework that maps from unit sphere surface to target surface, given a single color image. Previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D shape by a mapping from a fixed parameter domain. In our framework, the mapping is progressively carried out by the parameterization network, given parameters that semantic network predicted from a single color image. Our framework can directly generate mesh as output and it also allows us to integrate mesh based operation (e.g. Laplacian smooth) and mesh related losses into the network. Experiments show that our method not only qualitatively
	produces mesh model more visually appealing, but also achieves lower 3D shape estimation error compared to the state-of-the-art.   
\end{abstract}